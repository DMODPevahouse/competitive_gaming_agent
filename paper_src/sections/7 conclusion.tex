\section{Conclusion}
\label{sec:conclusion}


To bring this research to a close, what was accomplished in this experiment was a novel idea to make a reinforcement learning agent in a video game with a complicated environment, in this case Starcraft II, capable of competing with a difficult opponent provide a
challenge and winning in some cases, here with the best of the models it would still maintain a 50 percent win rate, but also dropping down to in skill to a less worthy opponent, like a random agent, and maintaining a 50 percent, or close, win ratio. 

This hypothesis showed promise in the fact that in several of the tests, such as alternating between best and half best, did reach near that goal over the course of 50 games against three opponents at nearly a 60 percent ratio, and with the non-alternating neutral 
the ratio was almost 40 percent. Meaning research could go even further for multiple degrees of competitive play. 

To test the hypothesis 3 agents were built, a random agent for training and pure random testing, a simple Q learning agent for a baseline as well as reliable results, and a DQN model. Each of these had their strengths and weaknesses but together applied a wide 
range of tests to experiment with the theory behind. Those tests were performed by the two trained agents against all three agents with 50 games per model state with competitive level and alternating level. 

To conclude, further research in this field is exciting and novel, which I hope to look into more as knowledge, computation, and time to spend on it increases. 