{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T17:03:27.342737Z",
     "start_time": "2024-08-17T17:02:50.134042Z"
    }
   },
   "cell_type": "code",
   "source": "!python FluxuatingSkillSC2.py",
   "id": "9741004caba5061c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Heirnetwork conversion",
   "id": "9e29ee3b0c8b4c7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, ob_space, act_space_array, activation=nn.ReLU()):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.map_width = 64\n",
    "        self.map_channels = C.MAP_CHANNELS\n",
    "\n",
    "        self.use_norm = False\n",
    "        self.sl_training = False\n",
    "\n",
    "        self.obs_layer = nn.Linear(ob_space, 256)\n",
    "        self.controller_layer1 = nn.Linear(256, 256)\n",
    "        self.controller_layer2 = nn.Linear(256, 64)\n",
    "\n",
    "        self.minimap_info_layer = nn.Conv2d(self.map_channels, 32, kernel_size=3)\n",
    "        self.minimap_info_layer2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.minimap_info_layer3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.minimap_info_layer4 = nn.Conv2d(64, 3, kernel_size=3)\n",
    "\n",
    "        self.battle_info_layer = nn.Linear(64 + 3 * 3, 256)\n",
    "        self.battle_probs_layer = nn.Linear(256, act_space_array[0])\n",
    "        self.battle_pos_probs_layer = nn.Linear(256, act_space_array[1])\n",
    "\n",
    "        self.value_layer1 = nn.Linear(ob_space, 256)\n",
    "        self.value_layer2 = nn.Linear(256, 128)\n",
    "        self.value_layer3 = nn.Linear(128, 128)\n",
    "        self.value_layer4 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, obs, map_data):\n",
    "        obs = F.relu(self.obs_layer(obs))\n",
    "        obs = F.relu(self.controller_layer1(obs))\n",
    "        obs = self.controller_layer2(obs)\n",
    "\n",
    "        map_data = F.relu(self.minimap_info_layer(map_data))\n",
    "        map_data = F.max_pool2d(map_data, kernel_size=2)\n",
    "        map_data = F.relu(self.minimap_info_layer2(map_data))\n",
    "        map_data = F.max_pool2d(map_data, kernel_size=2)\n",
    "        map_data = F.relu(self.minimap_info_layer3(map_data))\n",
    "        map_data = F.max_pool2d(map_data, kernel_size=2)\n",
    "        map_data = F.relu(self.minimap_info_layer4(map_data))\n",
    "        map_data = map_data.view(-1, 3 * 3)\n",
    "\n",
    "        battle_info = torch.cat((obs, map_data), dim=1)\n",
    "        battle_info = F.relu(self.battle_info_layer(battle_info))\n",
    "        battle_probs = self.battle_probs_layer(battle_info)\n",
    "        battle_probs = F.softmax(battle_probs, dim=1)\n",
    "\n",
    "        battle_pos_probs = self.battle_pos_probs_layer(battle_info)\n",
    "        battle_pos_probs = F.softmax(battle_pos_probs, dim=1)\n",
    "\n",
    "        value = F.relu(self.value_layer1(obs))\n",
    "        value = F.relu(self.value_layer2(value))\n",
    "        value = F.relu(self.value_layer3(value))\n",
    "        value = self.value_layer4(value)\n",
    "\n",
    "        return battle_probs, battle_pos_probs, value\n",
    "\n",
    "    def get_action(self, obs, map_data):\n",
    "        battle_probs, battle_pos_probs, _ = self.forward(obs, map_data)\n",
    "        battle_act = torch.multinomial(battle_probs, num_samples=1)\n",
    "        battle_pos = torch.multinomial(battle_pos_probs, num_samples=1)\n",
    "        return battle_act, battle_pos\n",
    "\n",
    "    def get_values(self, obs, map_data):\n",
    "        _, _, value = self.forward(obs, map_data)\n",
    "        return value\n",
    "\n",
    "class PPOTrain:\n",
    "    def __init__(self, policy, old_policy, gamma=0.995, clip_value=0.2, c_1=0.01, c_2=1e-6, epoch_num=20):\n",
    "        self.policy = policy\n",
    "        self.old_policy = old_policy\n",
    "        self.gamma = gamma\n",
    "        self.clip_value = clip_value\n",
    "        self.c_1 = c_1\n",
    "        self.c_2 = c_2\n",
    "        self.epoch_num = epoch_num\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=P.lr, eps=1e-5)\n",
    "\n",
    "    def get_loss(self, obs, map_data, battle_actions, battle_pos, gaes, rewards, v_preds_next):\n",
    "        battle_probs, battle_pos_probs, v_preds = self.policy.forward(obs, map_data)\n",
    "        act_probs = battle_probs * torch.one_hot(battle_actions, battle_probs.shape[1])\n",
    "        act_probs = act_probs.sum(dim=1)\n",
    "        act_probs_old = self.old_policy.forward(obs, map_data)[0] * torch.one_hot(battle_actions, battle_probs.shape[1])\n",
    "        act_probs_old = act_probs_old.sum(dim=1)\n",
    "\n",
    "        ratios = torch.exp(torch.log(act_probs) - torch.log(act_probs_old))\n",
    "                clipped_ratios = torch.clamp(ratios, 1 - self.clip_value, 1 + self.clip_value)\n",
    "        loss_clip = torch.minimum(gaes * ratios, gaes * clipped_ratios)\n",
    "        loss_clip = -loss_clip.mean()\n",
    "\n",
    "        battle_act_entropy = -torch.sum(battle_probs * torch.log(battle_probs), dim=1)\n",
    "        battle_pos_entropy = -torch.sum(battle_pos_probs * torch.log(battle_pos_probs), dim=1)\n",
    "        entropy = battle_act_entropy + battle_pos_entropy\n",
    "        entropy = entropy.mean()\n",
    "\n",
    "        loss_vf = (v_preds - rewards - self.gamma * v_preds_next) ** 2\n",
    "        loss_vf = loss_vf.mean()\n",
    "\n",
    "        total_loss = loss_clip + self.c_1 * loss_vf - self.c_2 * entropy\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, obs, map_data, battle_actions, battle_pos, gaes, rewards, v_preds_next):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.get_loss(obs, map_data, battle_actions, battle_pos, gaes, rewards, v_preds_next)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def assign_policy_parameters(self):\n",
    "        for param, old_param in zip(self.policy.parameters(), self.old_policy.parameters()):\n",
    "            old_param.data.copy_(param.data)\n",
    "\n",
    "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
    "        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(gaes) - 1)):\n",
    "            gaes[t] = gaes[t] + self.gamma * self.lamda * gaes[t + 1]\n",
    "        return gaes\n",
    "\n",
    "    def ppo_train_dis(self, observations, map_data, battle_actions, battle_pos, rewards, v_preds, v_preds_next, gaes, returns):\n",
    "        observations = torch.tensor(observations, dtype=torch.float32)\n",
    "        map_data = torch.tensor(map_data, dtype=torch.float32)\n",
    "        battle_actions = torch.tensor(battle_actions, dtype=torch.int64)\n",
    "        battle_pos = torch.tensor(battle_pos, dtype=torch.int64)\n",
    "        gaes = torch.tensor(gaes, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        v_preds_next = torch.tensor(v_preds_next, dtype=torch.float32)\n",
    "\n",
    "        for epoch in range(self.epoch_num):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.train(observations, map_data, battle_actions, battle_pos, gaes, rewards, v_preds_next)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.assign_policy_parameters()\n",
    "        return loss"
   ],
   "id": "a3f6f108ad84e83f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f99c4975a4d3ffc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
